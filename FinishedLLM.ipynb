{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "830bcc5c-28dd-4f0f-a2ce-933738a777f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahh, Hot-H\n",
      "[23, 55, 55, 7, 1, 30, 62, 67, 8, 30]\n",
      "Ahh, Hot-H\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "\n",
    "# Initialize a seed for generating random numbers\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "#Loading the training data\n",
    "#1337 is common to use, but it would be anything. \n",
    "inputText = 'LilWayneLyricsTrainingSet.txt'\n",
    "with open(inputText, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Set the unique charcters to variables. List() gets only unique\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(text[:10])\n",
    "print(encode(text[:10]))\n",
    "print(decode(encode(text[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc99bd4-f205-409d-b27d-b12d3db1f870",
   "metadata": {},
   "source": [
    "# Example of Encode Decode\n",
    "\n",
    "### Input\n",
    "- print(text[:10]) - Here i tring the first 10 characters of the training set\n",
    "- print(encode(text[:10])) - Here I encode and print the first 10 characters\n",
    "- print(decode(encode(text[:10]))) - Here I decode the encoded characters\n",
    "### Output\n",
    "- Ahh, Hot-H\n",
    "- [23, 55, 55, 7, 1, 30, 62, 67, 8, 30]\n",
    "- Ahh, Hot-H\n",
    "\n",
    "#### You can see from this example how encoding and decoding the characters worksH\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f00efafd-3aa0-4a4b-a542-5dd99b44884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.712141 M parameters\n",
      "step 0: train loss 4.5333, val loss 4.5342\n",
      "step 500: train loss 1.9465, val loss 2.1218\n",
      "step 1000: train loss 1.7172, val loss 2.0260\n",
      "step 1500: train loss 1.5511, val loss 1.9526\n",
      "step 2000: train loss 1.4030, val loss 1.9767\n",
      "step 2500: train loss 1.2649, val loss 1.9471\n",
      "step 3000: train loss 1.1550, val loss 1.9772\n",
      "step 3500: train loss 1.0566, val loss 2.0005\n",
      "step 4000: train loss 0.9579, val loss 2.0307\n",
      "step 4500: train loss 0.8836, val loss 2.0557\n",
      "step 4999: train loss 0.8089, val loss 2.1306\n",
      "\n",
      "Your lubressedle spitt drink 22\" writing\n",
      "Now, go Dirtore the han ble fuck and Cita—Coup an(T have I havi!\n",
      "Phat eest, I with I'm bear\n",
      "Plaze I go fuck 'Wayne is build, bitch\n",
      "Some say sweet him, chrome and the night She neez the can suite me, bitch\n",
      "And I'mma blazing ling Goding, but I aing baby di times ocen the car angel?\n",
      "When  gave everybody it will don't reder so be check\n",
      "Shood go gonna be? So h, drop it like it like itch\n",
      "Savide it's hitch the being sunring carked up\n",
      "Young Srite kur you wanna fu\n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# here we set the training size to 85% of the data set, and the validation data to 15% of the data set\n",
    "n = int(0.85*len(data)) \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "\n",
    "# Here we are loading the data\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    #This is a random starting point\n",
    "    randInt = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in randInt])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in randInt])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "#disable gradient compoutation, which helps with efficiency\n",
    "@torch.no_grad()\n",
    "\n",
    "\n",
    "def estimate_loss():\n",
    "    #initialize dictionary\n",
    "    out = {}\n",
    "    #Set the model to evaluation mode which affects the dropout and batchnorm,\n",
    "    model.eval()\n",
    "\n",
    "    #Here we loop over the training and validation datasets\n",
    "    for split in ['train', 'val']:\n",
    "        #The losses tensor stores losses for each iteration\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            #gets a batch of data\n",
    "            X, Y = get_batch(split)\n",
    "            #pass forward through the model to get logits and loss\n",
    "            logits, loss = model(X, Y)\n",
    "            #store loss\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        #Calculate and store the man loss\n",
    "        out[split] = losses.mean()\n",
    "    #set the model back to train\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# Represents a HEad of a self-attention mechanism\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #creates the linear layer key\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        #creates the linear layer for query\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        #creathes the linear layer for value\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        #registers a buffer named tril which is a lower triangular matrix of block_size x block_size\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "#defines the model\n",
    "class WayneLLM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "\n",
    "        #creates an embedding layer for the tokens, mapping each token in the vocabulary to a high-dimensional vector\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        #embedding layer for token positions\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        #A sequence of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        #layer normalization applied to the final output\n",
    "        self.ln_f = nn.LayerNorm(n_embd) \n",
    "\n",
    "        #linear layer to project from the dimensionality of embeddings to the vocabulary size\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Applies a custom weight initialization function to all modules\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    #custom weight initialization\n",
    "    def _init_weights(self, module):\n",
    "        #normal initialization for weights of linear layers\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # zero initialization for biases\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Normal initialization for weights of embedding layers\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # Forward pass of the model\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape #batch size B and sequence length T\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # get embeddings for each token and position\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb #Sum token and position embeddings (B,T,C)\n",
    "        x = self.blocks(x) # pass through transformer blocks(B,T,C)\n",
    "        x = self.ln_f(x) #Apply final layer normalization (B,T,C)\n",
    "        logits = self.lm_head(x) # project to vocabulary size (B,T,vocab_size)\n",
    "\n",
    "\n",
    "        #compute loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    #Generates new tokens given an initial context\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887c313-6aaf-4e88-a2e1-f029b38d40bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0a75d41-e39b-4645-8d82-234f4b7e213f",
   "metadata": {},
   "source": [
    "## First Output:\n",
    "LtS  oosbE jednjRf tte sogyoeer ti I thrusozgd toht roit miltl outo nrudruId, gskthRfo ; hBKchausis Jo unt t I,e cratQde odigddA\n",
    "coy\n",
    "ywHTl I dd y\n",
    "xlIrnEHlnata'the wal\n",
    "Fd&I kthf\n",
    "afeelyad T tuorin,rth theCrNrpllrmo YJherbthes;Codr s, wlu rooriNTiliconkcater.ald\n",
    "h, er rle sonot Is owirbly lhit wplithesnwrus: I t.I taRyvvusn.e thues the, rt\n",
    "y,tgndrirdn n$y s$\n",
    "Bortr.\n",
    "Yt sle lrctB tot sreerut polSh AenortyatTranulhvuthey'lot'oomedO, and oglawiithticrera:m.Y;odt\n",
    "t wadidrE:oqhe?m wtopy\n",
    "zun wt,oigthe d r t, minos oreFybourre e Bod sT giwoBsnstha!Ps Aodsthan wl gs lVxylhfor bllnocI alm.Ac;y ngris tha,sts, g rTrpistis'ped Mge tifdl tgroue c&d uldPv poNTheso yotQ,yrrleltJod$ agrw HeedY; averist Bkmmo? M whangh :ond&e wH grrfhi'ein Gw wthfatM od y pess Td thenom thasRYw wharyohi,eosd  sis ouao r wNoTus trlre am:Rn.OaukZra3Dluritor-s Q qZfdcsf:e shV\n",
    "tpd nghrtthoshouEncw tiXt ; ontinsuson t bgs gt :e touooErit t mhT't.\n",
    "-s e thOo  Ieust i t s craeree lofdsore Ou\n",
    "SntI, t trazw,Viscea,,J m, IaEin.Ialn OcbbzoRaPk bsthNikr'! bhdiudorlt3 gcrn gri!sthecd wyIRhoGt ilu it pSdroy.? g klor e I'o TimYwdo;d tetg thagsr crt,\n",
    "l Lese, ldrmoM bSKnd sths\n",
    "UAd\n",
    "Gol wyOiiursheplre gatin Q &dl aSUf i,tdnous&s.esit,Kd\n",
    "Aso Og 'th t\n",
    "Jcd re.Wygo hir aHVdTherdRJ vn! teth oHocuiGthesoo: tcrt AimhwTUgc;. t watr stheathe ithe$. rint\n",
    "mufhaW tu&rteli'apaidnp !scur Loan-clkgliothiCd l rg oiue chd thpmog mesdsriuanreorthadnDd atrem dsr s kssl Itholto utmrer otahecralTod,ssd\n",
    "Mlll r t sdc?oil; thedruD CgatwThdy :.inI bl unthayo\n",
    "\n",
    "d er pthalonrk thuF f.\n",
    "TheLusinrourilmuorhllYoharlar Kskanteraa&mu,F' baead nhethoDetS e ikiCn, ind.i$ ';Al jhil  ws Thp d !rdt DngrtCIn d: tdATvf kKnaiMipe'sthsIh'aiuthainorat :,ime srlgumis todond fg BhmongyotheowhtheuNwu;oumeguy moiprsn miun w oteg s&l oul: waoaLturkul.cfdfd::r\n",
    "\n",
    "irdy t; khI l\n",
    "\n",
    "T wecotnoAer iToy lt sset oarat sarator;h thhemedrsld ogyd arno uiu ae \n",
    "st miZ n$es\n",
    "re tidVdro a,s w yecothdr f\n",
    "ss thavfgoogrewnwr s hori Uyeilowed wrionouAav.e y COa!,YohmonomYsysNnthorthre ?hanc li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9d4f0-3c25-4732-8f50-6ef39040a91f",
   "metadata": {},
   "source": [
    "# Initial training with shakespear dataset\n",
    "## Here is the parameters losses\n",
    " - 10.702913 M parameters\n",
    " - step 0: train loss 4.1498, val loss 4.1521\n",
    " - step 500: train loss 2.1906, val loss 2.2049\n",
    " - step 1000: train loss 2.0205, val loss 2.0848\n",
    " - step 1500: train loss 1.9077, val loss 2.0109\n",
    " - step 2000: train loss 1.8310, val loss 1.9509\n",
    " - step 2500: train loss 1.7718, val loss 1.9182\n",
    " - step 3000: train loss 1.7225, val loss 1.8872\n",
    " - step 3500: train loss 1.6758, val loss 1.8286\n",
    " - step 4000: train loss 1.6619, val loss 1.8261\n",
    " - step 4500: train loss 1.6265, val loss 1.7787\n",
    " - step 4999: train loss 1.6132, val loss 1.7725\n",
    "\n",
    "\n",
    "## Here is the ouput of 500 characters\n",
    "```\n",
    "GLOUCESTER:\n",
    "Yet got there speak; glittless, I donne's fatter\n",
    "Of our peoffort; a lathlike grust time.\n",
    "Dawerst your me, as decraid her day your wind she fet vence pleason senders.\n",
    "\n",
    "WAGREMNENIA:\n",
    "My lord: growes yould, No,\n",
    "Excurdease, a let yet so take he fears;\n",
    "And help's figer, being in plearies'.\n",
    "\n",
    "GLOUCESTER:\n",
    "Fortwe though and my all plucier\n",
    "Sity becone cannot, which nor let she but donoth thee rangerous othand that\n",
    "What the revenisons and my bedin, not so goods,\n",
    "Andim fiveing the gentle eaven wa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108dde2-9e0c-4c08-b715-386be794af0f",
   "metadata": {},
   "source": [
    "# Initial training with Lil Wayne Dataset\n",
    "\n",
    "## Here are the parameters and losses:\n",
    "\n",
    "- 10.712141 M parameters\n",
    "- step 0: train loss 4.5333, val loss 4.5342\n",
    "- step 500: train loss 1.9465, val loss 2.1218\n",
    "- step 1000: train loss 1.7172, val loss 2.0260\n",
    "- step 1500: train loss 1.5511, val loss 1.9526\n",
    "- step 2000: train loss 1.4030, val loss 1.9767\n",
    "- step 2500: train loss 1.2649, val loss 1.9471\n",
    "- step 3000: train loss 1.1550, val loss 1.9772\n",
    "- step 3500: train loss 1.0566, val loss 2.0005\n",
    "- step 4000: train loss 0.9579, val loss 2.0307\n",
    "- step 4500: train loss 0.8836, val loss 2.0557\n",
    "- step 4999: train loss 0.8089, val loss 2.1306\n",
    "### Here is the output from 500 characters:\n",
    "```\n",
    "Your lubressedle spitt drink 22\" writing\n",
    "Now, go Dirtore the han ble fuck and Cita—Coup an(T have I havi!\n",
    "Phat eest, I with I'm bear\n",
    "Plaze I go fuck 'Wayne is build, bitch\n",
    "Some say sweet him, chrome and the night She neez the can suite me, bitch\n",
    "And I'mma blazing ling Goding, but I aing baby di times ocen the car angel?\n",
    "When  gave everybody it will don't reder so be check\n",
    "Shood go gonna be? So h, drop it like it like itch\n",
    "Savide it's hitch the being sunring carked up\n",
    "Young Srite kur you wanna fu\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0607443-2c05-4b05-a10a-41d31285fa84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bcc76c-93a7-450a-8349-aff37c3df1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
